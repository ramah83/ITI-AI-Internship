{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e1dc2ec36a962ef67adee71914d4c1bd",
     "grade": false,
     "grade_id": "cell-1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Week 6 - Classification\n",
    "\n",
    "This week's assignment will give you an opportunity to practice your classification skills with the scikit-learn package and learn to implement your own sklearn-like classifier from scratch.\n",
    "\n",
    "In the first component, you will apply logistic regression to a dataset of wine quality. The idea is to predict wine quality based on its chemical properties. Read more about the wine dataset here - https://archive.ics.uci.edu/ml/datasets/wine+quality.\n",
    "\n",
    "In the second component, you will use multinomial naive Bayes to predict wine qualities.\n",
    "\n",
    "In the third component, you will implement your own version of KNN.\n",
    "\n",
    "Note: Due to a heavy use of randomization techniques the exact performance mertics may not be achieved (sometimes even with the random seed set) therefore the tests in assert statements will  validate your solutions roughly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6d9e5613e9b2c951f67ffc08d594dbcd",
     "grade": false,
     "grade_id": "cell-2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "94ba21507e17816c123cf86bb7f0f5ac",
     "grade": false,
     "grade_id": "cell-3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 1 (1 point) - Logistic Regression\n",
    "\n",
    "In this task you will use RepeatedStratifiedKfold cross-validation to evaluate performance of Logistic Regression on the wine dataset, predicting wine quality.\n",
    "\n",
    "Wine quality can take values between 3-8. However, for this task you need to convert these qualities to a binary value and your model will predict whether a specific wine is \"high quality\". We will define wine with a quality value of __7__ and above as high-quality.\n",
    "\n",
    "In order to get an estimate of how well your model is doing, you will use RepeatedStratifiedKfold cross-validation. This is an sklearn cross-validation iterator that runs stratified K-fold cross-valdiation multipe times.\n",
    "\n",
    "We will assess how well we are doing using __accuracy__. To do this using the cross_val_score function, set the \"scoring\" parameter to 'accuracy' (scoring='accuracy').\n",
    "\n",
    "Store the wine features in a variable named \"X\", and the response in a variable named \"y\". Store the cross-validation scores in a \"scores\" variable. We will check these variables in our asserts.\n",
    "\n",
    "Additional instructions:\n",
    "* Use the following from sklearn:  RepeatedStratifiedKFold, LogisticRegression and cross_val_score.\n",
    "* Specify solver='liblinear' in LogisticRegression()\n",
    "* We will load the wine dataset for you. You will need to divide the wine dataset into X and y.\n",
    "* X should be a matrix of all the features in the dataset, __excluding wine quality__.\n",
    "* y should be a vector of 0s and 1s, with 0 denoting low-quality wines and 1 denoting high-quality wines.\n",
    "* Initialize the Repeated K-fold cross validation object to have 5 folds and 10 repetitions. __Set random_state to 42__.\n",
    "* For a visual aid, you can plot the distribution of scores across the cross-validation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "07f6bf64ae6d0d55f00cea3b4b6d4078",
     "grade": false,
     "grade_id": "cell-4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Here we are loading the wine data for you.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wines = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "pass\n",
    "\n",
    "# To view scores you can run: pd.Series(scores).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aa657ecaa7603fe433ee77e1419cfc72",
     "grade": true,
     "grade_id": "cell-5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert y.shape == (1599,)\n",
    "assert X.shape == (1599, 11)\n",
    "assert  pd.Series(scores).mean() > 0.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "280c822999661ead532d9f5e925c2891",
     "grade": false,
     "grade_id": "cell-6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 2 (1 point) - Multinomial Naive Bayes\n",
    "\n",
    "In this task you will use RepeatedStratifiedKfold cross-validation to evaluate performance of multinomial naive Bayes (NB) on the wine dataset, predicting wine quality. This is similar to the previous task, except here we are using multinomial NB and we are not converting the wine qualities to binary.\n",
    "\n",
    "For this task, we've decided we want to only use wines with qualities between __5__ and __7__ (inclusive). You will have to exclude wines that are not in this range.\n",
    "\n",
    "In order to get an estimate of how well your model is doing, you will use RepeatedStratifiedKfold cross-validation. This is an sklearn cross-validation iterator that runs stratified K-fold cross-valdiation multipe times.\n",
    "\n",
    "We will assess how well we are doing using __f1_macro__. f1_macro is an evaluation based on the f1 score, but it can be used in multiclass (non-binary) problems. To do this using the cross_val_score function, set the \"scoring\" parameter to 'f1_macro' (scoring='f1_macro').\n",
    "\n",
    "Store the wine features in a variable named \"X\", and the response in a variable named \"y\". Store the cross-validation scores in a \"scores\" variable. We will check these variables in our asserts.\n",
    "\n",
    "Additional instructions:\n",
    "* Use the following from sklearn:  RepeatedStratifiedKFold, MultinomialNB and cross_val_score.\n",
    "* We will load the wine dataset for you. You will need to divide the wine dataset into X and y.\n",
    "* X should be a matrix of all the features in the dataset, __excluding the wine quality feature__ and __excluding wines with qualities that are less than 5 or greater than 7__.\n",
    "* y should be a vector of wine qualities ranging from 5 to 7 (inclusive).\n",
    "* Initialize the Repeated K-fold cross validation object to have 5 splits and 1000 repetitions. __Set random_state to 42__.\n",
    "* For a visual aid, you can plot the distribution of scores across the cross-validation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "26a70002d47058c345003f24ce8b7c54",
     "grade": false,
     "grade_id": "cell-7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wines = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "# YOUR CODE HERE\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "322f2a5d9a03bf59073cb0dda276d528",
     "grade": true,
     "grade_id": "cell-8",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert y.shape == (1518,)\n",
    "assert X.shape == (1518, 11)\n",
    "assert  .4 < pd.Series(scores).mean() < .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5d90c6a36aea4d3b36fa961a397c25ff",
     "grade": false,
     "grade_id": "cell-9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Task 3  (2 points) - KNN\n",
    "\n",
    "Implement your own version of Nearest Neighbor classifier as a class named MyKNN. The only hyperparameter to \\_\\_init\\_\\_(self, K) is K - the number of neighbors.\n",
    "\n",
    "Implement .fit(X, y) method. There is no real training as the model simply memorizes all the data samples and their class labels.\n",
    "\n",
    "Implement .predict(X_new) method; this is where all the calculations are. You need to compare each sample in X_new to the memorized data and choose the K nearest neighbors using euclidean distance. Then you need to predict the label based on the most frequent label of the K neighbors and return an array of predicted labels. The problem could be binary or multiclass, it does not matter for the implementation, however you should predict only one label for each sample.\n",
    "\n",
    "Do not worry about memory or speed optimization, we will not use MyKNN on large datasets.\n",
    "\n",
    "Hints:\n",
    "\n",
    "    - .fit(X) will “memorize” X\n",
    "    \n",
    "    - .predict(V) algorithm has three main steps:\n",
    "        1. calculate distance from every example in V to every example in memorized X\n",
    "        2. for every example v in V find the closest K neighbors in memorized X based on these distances (need to sort by distance)\n",
    "        3. then take the labels of these K neighbors and assign the most frequent label to example v. As a result we predicted a label for each example in V.\n",
    "        \n",
    "    - sklearn.metrics.pairwise.euclidean_distances or scipy.spatial.distance.euclidean could be helpful \n",
    "\n",
    "    -   If you want your code to be concise — use vectorized functions in numpy and scipy:\n",
    "        scipy.spatial.distance.cdist\n",
    "        numpy.argsort\n",
    "        numpy.take\n",
    "        scipy.stats.mode\n",
    "        Otherwise, a plain old python would do the job too.\n",
    "\n",
    "    - In scikit-learn NearestNeighbors provides the basic functionality\n",
    "        Try to avoid using NearestNeighbors directly, but you may if you get completely stuck. https://scikit-learn.org/stable/modules/neighbors.html\n",
    "\n",
    "    - You are not allowed to use KNeighborsClassifier, that’s the whole point of the exercise to implement its functionality.\n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "        You can still use the available code in scikit-learn as inspiration, i.e. copying ideas and even pieces of code, if necessary.\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "13af79d3e54332587f3746f6c92d2ab0",
     "grade": false,
     "grade_id": "t1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# follow this class template:\n",
    "class MyKNN(object):\n",
    "    def __init__(self, K=1):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "# YOUR CODE HERE\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "067fbc1556e4e698bd49ef6949ee7ed2",
     "grade": true,
     "grade_id": "t1-test",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "##### Iris dataset ##############################################\n",
    "# 150 samples, 4 features\n",
    "# 3 classes, 50 samples per class\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X_iris, y_iris = load_iris(return_X_y=True)\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(X_iris, y_iris, test_size=0.3, stratify=y_iris)\n",
    "cls_iris = MyKNN(5)\n",
    "cls_iris.fit(X_iris_train, y_iris_train)\n",
    "\n",
    "ref_cls_iris = KNeighborsClassifier(5)\n",
    "ref_cls_iris.fit(X_iris_train, y_iris_train)\n",
    "assert accuracy_score(y_iris_test, ref_cls_iris.predict(X_iris_test)) >= 0.9\n",
    "assert accuracy_score(y_iris_test, cls_iris.predict(X_iris_test)) >= 0.9"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
